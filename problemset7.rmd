---
title: 'IMT 573: Problem Set 7 - Regression - Solutions'
author: "Wenyue Yin"
date: 'Due: Tuesday, November 19, 2019'
output: pdf_document
header-includes:
- \newcommand{\benum}{\begin{enumerate}}
- \newcommand{\eenum}{\end{enumerate}}
- \newcommand{\bitem}{\begin{itemize}}
- \newcommand{\eitem}{\end{itemize}}
---

<!-- This syntax can be used to add comments that are ignored during knitting process. -->

##### Collaborators: Tianyi Zhou

##### Instructions:

Before beginning this assignment, please ensure you have access to R and RStudio; this can be on your own personal computer or on the IMT 573 R Studio Server. 

1. Download the `problemset7.Rmd` file from Canvas or save a copy to your local directory on RStudio Server. Open `problemset7.Rmd` in RStudio and supply your solutions to the assignment by editing `problemset7.Rmd`. 

2. Replace the "Insert Your Name Here" text in the `author:` field with your own full name. Any collaborators must be listed on the top of your assignment. 

3. Be sure to include well-documented (e.g. commented) code chucks, figures, and clearly written text chunk explanations as necessary. Any figures should be clearly labeled and appropriately referenced within the text. Be sure that each visualization adds value to your written explanation; avoid redundancy -- you do not need four different visualizations of the same pattern.

4.  Collaboration on problem sets is fun and useful, and we encourage it, but each student must turn in an individual write-up in their own words as well as code/work that is their own.  Regardless of whether you work with others, what you turn in must be your own work; this includes code and interpretation of results. The names of all collaborators must be listed on each assignment. Do not copy-and-paste from other students' responses or code.

5. All materials and resources that you use (with the exception of lecture slides) must be appropriately referenced within your assignment.  

6. Remember partial credit will be awarded for each question for which a serious attempt at finding an answer has been shown. Students are \emph{strongly} encouraged to attempt each question and to document their reasoning process even if they cannot find the correct answer. If you would like to include R code to show this process, but it does not run without errors, you can do so with the `eval=FALSE` option. (Note: I am also using the `include=FALSE` option here to not include this code in the PDF, but you need to remove this or change it to `TRUE` if you want to include the code chunk.)

```{r example chunk with a bug, eval=FALSE, include=FALSE}
a + b # these object dont' exist 
# if you run this on its own it with give an error
```

7. When you have completed the assignment and have **checked** that your code both runs in the Console and knits correctly when you click `Knit PDF`, rename the knitted PDF file to `ps7_YourLastName_YourFirstName.pdf`, and submit the PDF file on Canvas.

##### Setup

In this problem set you will need, at minimum, the following R packages.

```{r Setup, message=FALSE}
# Load standard libraries
library(tidyverse)
library(MASS) # Modern applied statistics functions
```

\textbf{Housing Values in Suburbs of Boston}

In this problem we will use the Boston dataset that is available in the \texttt{MASS} package. This dataset contains information about median house value for 506 neighborhoods in Boston, MA. Load this data and use it to answer the following questions.

```{r}
data("Boston")
Data <- tbl_df(Boston) 
```

\benum

\item Describe the data and variables that are part of the \texttt{Boston} dataset. Tidy data as necessary.

```{r}
#crim: per capita crime rate by town.

#zn: proportion of residential land zoned for lots over 25,000 sq.ft.

#indus: proportion of non-retail business acres per town.

#chas: Charles River dummy variable (= 1 if tract bounds river; 0 otherwise).

#nox: nitrogen oxides concentration (parts per 10 million).

#rm: average number of rooms per dwelling.

#age: proportion of owner-occupied units built prior to 1940.

#dis: weighted mean of distances to five Boston employment centres.

#rad: index of accessibility to radial highways.

#tax: full-value property-tax rate per \$10,000.

#ptratio: pupil-teacher ratio by town.

#black: 1000(Bk - 0.63)^2 where Bk is the proportion of blacks by town.

#lstat: lower status of the population (percent).

#medv: median value of owner-occupied homes in \$1000s.
```
\item Consider this data in context, what is the response variable of interest?
  
```{r}
#medv
```
  
\item For each predictor, fit a simple linear regression model to predict the response. In which of the models is there a statistically significant association between the predictor and the response? Create some plots to back up your assertions. 

```{r}
#model = lm(crim ~ zn, Data)
#p <- summary(model)$coefficient[2,4]

currentRegression <- character()
possibleRegression<- names(Data[1,1:13])
Pvalue <- 0.05
test <- data.frame()

for (var in possibleRegression){
  temp <- currentRegression
  print(temp)
  temp[length(temp) +1] <- var
  temp<- c('medv', temp)
  currentlm <- lm(medv~. , Data[, temp])
  currentp <- summary(currentlm)$coefficient[2,4]
  test1 <- data.frame(var, currentp)
  test <- rbind(test, test1)
  print(test)
}



#All models have statistically significant association between the predictor and the response.
```

```{r}
plot(test)
```

\item Fit a multiple regression model to predict the response using all of the predictors. Describe your results. For which predictors can we reject the null hypothesis $H_0: \beta_j = 0$?

```{r}
AssoAll <- lm(medv ~., Data)
summary(AssoAll)
# For all predictors except indus and age, we can reject the null hypothesis H0 = 0.
```

\item How do your results from (3) compare to your results from (4)? Create a plot displaying the univariate regression coefficients from (3) on the x-axis and the multiple regression coefficients from part (4) on the y-axis. Use this visualization to support your response.

```{r}
#There are some different values between result from (3) and (4).
co_multi <- summary(AssoAll)$coefficient[,4]
co_multi <- as.data.frame(co_multi)
co_multi <- co_multi[-1,]
joined <- cbind(test,co_multi)
ggplot(joined, aes(x = joined$currentp, y = joined$co_multi))+
  geom_point(xlim = "univariate regression coefficients", ylim = "multiple regression coefficients")
```

\item Is there evidence of a non-linear association between any of the predictors and the response? To answer this question, for each predictor $X$ fit a model of the form:
  
  $$ Y = \beta_0 + \beta_1 X + \beta_2 X^2 + \beta_3 X^3 + \epsilon $$

```{r}
summary(lm(medv~ crim + I(crim^2)+ I(crim^3), data = Data))
```  
```{r}
summary(lm(medv~ zn + I(zn^2)+ I(zn^3), data = Data))
```  
```{r}
summary(lm(medv~ indus + I(indus^2)+ I(indus^3), data = Data))
```  
```{r}
summary(lm(medv~ chas + I(chas^2)+ I(chas^3), data = Data))
```  
```{r}
summary(lm(medv~ nox + I(nox^2)+ I(nox^3), data = Data))
```  
```{r}
summary(lm(medv~ rm + I(rm^2)+ I(rm^3), data = Data))
```  
```{r}
summary(lm(medv~ age + I(age^2)+ I(age^3), data = Data))
```  
```{r}
summary(lm(medv~ dis + I(dis^2)+ I(dis^3), data = Data))
```  
```{r}
summary(lm(medv~ rad + I(rad^2)+ I(rad^3), data = Data))
```  
```{r}
summary(lm(medv~ tax + I(tax^2)+ I(tax^3), data = Data))
```  
```{r}
summary(lm(medv~ ptratio + I(ptratio^2)+ I(ptratio^3), data = Data))
```  
```{r}
summary(lm(medv~ black + I(black^2)+ I(black^3), data = Data))
```  
```{r}
summary(lm(medv~ lstat + I(lstat^2)+ I(lstat^3), data = Data))
```  
```{r}
# for crim, nox, indus, zn, rm, dis, rad, lstat , there is evidence of a non-linear relationship, as each of these variables squared and cubed terms is found to be statistically significant.
```  



\item Consider performing a stepwise model selection procedure to determine the bets fit model. Discuss your results. How is this model different from the model in (4)?

```{r}
stepwise <- stepAIC(AssoAll, direction = "backward")
stepwise
#The predictors left are the same
```  
\item Evaluate the statistical assumptions in your regression analysis from (7) by performing a basic analysis of model residuals and any unusual observations. Discuss any concerns you have about your model.

```{r}
par(mfrow = c(2,2))
plot(stepwise)
#the residual plot shows a curvature, showing that there are some non-linerity in the model.
```
\eenum


